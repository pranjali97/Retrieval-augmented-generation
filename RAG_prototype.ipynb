{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c8c602eb-5319-458d-a827-35f36b2adac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ad33dd72-a66d-4f2e-98ec-e513f3259d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine, euclidean\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class VectorDatabase:\n",
    "    def __init__(self):\n",
    "        self.database = []\n",
    "        \n",
    "    def insert(self, entries: List[Dict[str, Any]]):\n",
    "        # insert id, text and vector into the DB\n",
    "        for entry in entries:\n",
    "            self.database.append((entry['id'], entry['metadata']['text'], entry['vector']))\n",
    "\n",
    "    def search(self, query_vector: np.ndarray, k=1, metric='cosine') -> List[str]:\n",
    "        # Retreive entries closest to query\n",
    "        distances = []\n",
    "        for _, text, vector in self.database:\n",
    "            if metric == 'cosine':\n",
    "                distance = cosine(query_vector, vector)\n",
    "            elif metric == 'dot':\n",
    "                distance = np.dot(query_vector, vector)\n",
    "            elif metric == 'euclidean':\n",
    "                distance = euclidean(query_vector, vector)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported metric\")\n",
    "            distances.append((distance, text))\n",
    "        \n",
    "        # Sort usingdistance\n",
    "        if metric in ['cosine', 'dot']:\n",
    "            distances.sort(key=lambda x: x[0], reverse=True)\n",
    "        else:\n",
    "            distances.sort(key=lambda x: x[0])\n",
    "            \n",
    "        return [text for _, text in distances[:k]]\n",
    "\n",
    "db = VectorDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2ef62d68-add3-4873-803f-6cf50207922a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TaylorAI/bge-micro\")\n",
    "model = AutoModel.from_pretrained(\"TaylorAI/bge-micro\")\n",
    "\n",
    "# encode function using the embedding model\n",
    "def encode(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.pooler_output.detach().numpy()[0]\n",
    "\n",
    "#initialize db\n",
    "with open('blog.json', 'r') as file: \n",
    "        blogs = json.load(file)\n",
    "        entries = [{'id': blog['id'], 'metadata': blog['metadata'], 'vector': encode(blog['metadata']['text'])} for blog in blogs]\n",
    "        db.insert(entries)\n",
    "    \n",
    "# query_vector = encode(\"What are the latest advancements in AI?\")\n",
    "# top_texts = db.search(query_vector, k=3, metric='cosine')\n",
    "# print(top_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "deb7b7de-0738-4632-a65a-eafd5d51f5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can I help you today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " what are llms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the context - This journey is 1% finished: One of our favorite slogans at Facebook was always, “this journey is only 1% finished”. The same is true for AI in general: the world is only just discovering its power. As founders, we are humbled by the support from our amazing team of investors, as well as our core team of rock star engineers, scientists and others.,We have seen firsthand how AI can have a positive impact on the world, and how gratifying it is to help state-of-the-art AI research turn into something concrete that actually makes the world a better place. We are excited to continue that journey at Contextual AI.,Our Journey: In our journeys as research leaders at top AI institutions, both in industry (Microsoft Research, Meta/Facebook AI Research, Hugging Face) and academia (Cambridge, NYU, Stanford), we have always been attracted to the most difficult problems. We first started working together at Facebook in 2016 where we built a multimodal framework, synthesizing information from text, images and video to help deal with especially difficult problems, such as detecting hate speech in memes, catching the sale of illicit goods and fighting misinformation. After Facebook, we both went to Hugging Face in early 2022, where we worked on large language model technology, multimodality, and pushed the envelope on model evaluation.. what are llms? We are currently working on a new language for the world's most popular social network, which will be used to help solve problems like hate speech, and to help solve problems like hate speech in general. We are also working on a new language for the world's most popular social network, which will be used to help solve problems like hate speech, and to help solve problems like hate speech in general. We are also working on a new language for the world's most popular social network, which will be used to help solve problems like hate speech, and to help solve problems like hate speech in general. We are also working on a new language for the world's most popular social network, which will be used to help solve problems like hate speech, and to help solve problems like hate speech in general. We are also working on a new language for the world's most popular social network, which will be used to help solve problems like hate speech, and to help solve problems like hate speech in general. We are also working on a new language for the world's most popular social\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Model\n",
    "import torch\n",
    "\n",
    "class RAGSystem:\n",
    "    def __init__(self, vector_database, model_name='gpt2'):\n",
    "        self.vector_database = vector_database\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "    def generate_response(self, query, k=3, search_metric='cosine'):\n",
    "        # Retrieve top k documents\n",
    "        query_vector = encode(query)\n",
    "        top_documents = self.vector_database.search(query_vector, k=k, metric=search_metric)\n",
    "        \n",
    "        # Create a prompt with query and context\n",
    "        prompt = 'Given the context - '+ \",\".join(top_documents) + \". \" + query\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "        # Generate using GPT-2\n",
    "        output_sequences = self.model.generate(input_ids, max_length=500, num_return_sequences=1, pad_token_id=self.tokenizer.eos_token_id)\n",
    "\n",
    "        # Decode the generation\n",
    "        generated_sequence = output_sequences[0]\n",
    "        text = self.tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
    "\n",
    "        return text\n",
    "\n",
    "def main():\n",
    "    print(\"How can I help you today?\")\n",
    "    query = input()\n",
    "    rag = RAGSystem(db)\n",
    "    response = rag.generate_response(query)\n",
    "    print(response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a426e7b0-32d7-46fb-b591-b5c8a9b2faa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
